{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "With this context, EDSA is challenging you during the Classification Sprint with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/wine.jpg\"\n",
    "     alt=\"Some fine wine for your fine model\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=600px/>\n",
    "Some fine wine for your fine modeling process. \n",
    "Photo by <a href=\"https://unsplash.com/@hermez777?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\"> Hermes Rivera</a> on Unsplash\n",
    "</div>\n",
    "\n",
    "The structure of this notebook is as follows:\n",
    "\n",
    " - First, we'll load our data to get a view of the predictor and response variables we will be modeling. \n",
    " - We'll then preprocess our data, binarising the target variable and splitting up the data intro train and test sets. \n",
    " - We then model our data using a Support Vector Classifier.\n",
    " - Following this modeling, we define a custom metric as the log-loss in order to evaluate our produced model.\n",
    " - Using this metric, we then take several steps to improve our base model's performance by optimising the hyperparameters of the SVC through a grid search strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# set plot style\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "df = pd.read_csv('C:/Users/Mpilenhle/Documents/EDSA/Classification/Advanced_Classification_Predict-student_data-2780/train.csv')\n",
    "df_test = pd.read_csv('C:/Users/Mpilenhle/Documents/EDSA/Classification/Advanced_Classification_Predict-student_data-2780/test_with_no_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset \n",
    "\n",
    "For this coding challenge we'll be using the [Wine Quality dataset](https://archive.ics.uci.edu/ml/datasets/wine+quality) from the UCI Machine Learning Repository. The constituents of this dataset are red and white variants of the Portuguese \"Vinho Verde\" wine. \n",
    "\n",
    "This dataset consists of the following variables: \n",
    "\n",
    " - sentiments\n",
    " - message\n",
    " - tweetid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Worth a read whether you do or don't believe i...</td>\n",
       "      <td>425577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @thenation: Mike Pence doesn’t believe in g...</td>\n",
       "      <td>294933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @makeandmendlife: Six big things we can ALL...</td>\n",
       "      <td>992717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>@AceofSpadesHQ My 8yo nephew is inconsolable. ...</td>\n",
       "      <td>664510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @paigetweedy: no offense… but like… how do ...</td>\n",
       "      <td>260471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @StephenSchlegel: she's thinking about how ...</td>\n",
       "      <td>295793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>I do hope people who are vocal about climate c...</td>\n",
       "      <td>763719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @tveitdal: We only have a 5 percent chance ...</td>\n",
       "      <td>454673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @Alifaith55: Oh. My. God.\\n\\nTrump's Govern...</td>\n",
       "      <td>41161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>Fossil fuel giant ExxonMobil ‘misled’ the publ...</td>\n",
       "      <td>658092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            message  tweetid\n",
       "0           1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1           1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2           2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3           1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4           1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954\n",
       "5           1  Worth a read whether you do or don't believe i...   425577\n",
       "6           1  RT @thenation: Mike Pence doesn’t believe in g...   294933\n",
       "7           1  RT @makeandmendlife: Six big things we can ALL...   992717\n",
       "8           1  @AceofSpadesHQ My 8yo nephew is inconsolable. ...   664510\n",
       "9           1  RT @paigetweedy: no offense… but like… how do ...   260471\n",
       "10          1  RT @StephenSchlegel: she's thinking about how ...   295793\n",
       "11          1  I do hope people who are vocal about climate c...   763719\n",
       "12          2  RT @tveitdal: We only have a 5 percent chance ...   454673\n",
       "13          1  RT @Alifaith55: Oh. My. God.\\n\\nTrump's Govern...    41161\n",
       "14          2  Fossil fuel giant ExxonMobil ‘misled’ the publ...   658092"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the data\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid\n",
       "0  Europe will now be looking to China to make su...   169760\n",
       "1  Combine this with the polling of staffers re c...    35326\n",
       "2  The scary, unimpeachable evidence that climate...   224985\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "\"\"\"\n",
    "The function uses some of the functions remove_emoji() which removes\n",
    "emojis in a tweet\n",
    "it also uses the function remove_punctuation() which removes\n",
    "punctuations\n",
    "\n",
    "The function data_cleaner() implements both these functions to make a \n",
    "clean data frame, with the use of the regular expressions it removes\n",
    "noise or unwanted charecters in the tweets\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#creating a function for removing emojis\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\" \n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "# punctuation remover function\n",
    "def remove_punctuation(tweets):\n",
    "    return ''.join([l for l in tweets if l not in string.punctuation])\n",
    "\n",
    "\n",
    "\n",
    "def data_cleaner(df, column):\n",
    "    \n",
    "    #remmoving the urls\n",
    "    pattern_url = r'http[s]?://t.co/[A-Za-z0-9]+'\n",
    "    subs_url = r'url-web'\n",
    "    df[column] = df[column].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "    \n",
    "    #remmoving the Re Tweets \n",
    "    pattern_url = r'RT\\s\\@[A-Za-z0-9_]+:'\n",
    "    subs_url = r''\n",
    "    df[column] = df[column].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "    \n",
    "\n",
    "    #remmoving the mentions \n",
    "    pattern_url = r'@[A-Za-z0-9_]+'\n",
    "    subs_url = r''\n",
    "    df[column] = df[column].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "\n",
    "\n",
    "    #remmoving the Hashtags \n",
    "    pattern_url = r'\\#[A-Za-z0-9#?_]+'\n",
    "    subs_url = r''\n",
    "    df[column] = df[column].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "\n",
    "\n",
    "    #remmoving the remaining https\n",
    "    pattern_url = r'https:[.*?]+'\n",
    "    subs_url = r''\n",
    "    df[column] = df[column].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "\n",
    "    # turning all tweets to lower case\n",
    "    df[column] = df[column].str.lower()\n",
    "    \n",
    "    # using apply method to remove the punctuation marks\n",
    "    df[column] = df[column].apply(remove_punctuation)\n",
    "    \n",
    "    # Removing the emojis using the apply method\n",
    "    df[column] = df[column].apply(remove_emoji)\n",
    "    \n",
    "    #remmoving the uknown charecters from words\n",
    "    pattern_url = r'\\ã¢â‚¬â¦ | \\ã¢â‚¬â„¢[a-z] | \\… | \\ã¢â‚¬â€œ | \\ã¢å¾â¡ã¯â¸ï†\\x8f'\n",
    "    subs_url = r''\n",
    "    df[column] = df[column].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_cleaner(df, 'message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>its not like we lack evidence of anthropogenic...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>researchers say we have three years to act on...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>wired  2016 was a pivotal year in the war on ...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>its 2016 and a racist sexist climate change d...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15814</th>\n",
       "      <td>1</td>\n",
       "      <td>they took down the material on global warming...</td>\n",
       "      <td>22001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15815</th>\n",
       "      <td>2</td>\n",
       "      <td>how climate change could be breaking up a 200...</td>\n",
       "      <td>17856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15816</th>\n",
       "      <td>0</td>\n",
       "      <td>notiven rt nytimesworld what does trump actual...</td>\n",
       "      <td>384248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15817</th>\n",
       "      <td>-1</td>\n",
       "      <td>hey liberals the climate change crap is a hoa...</td>\n",
       "      <td>819732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15818</th>\n",
       "      <td>0</td>\n",
       "      <td>s climate change equation in 4 screenshots ur...</td>\n",
       "      <td>806319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15819 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            message  tweetid\n",
       "0              1  polyscimajor epa chief doesnt think carbon dio...   625221\n",
       "1              1  its not like we lack evidence of anthropogenic...   126103\n",
       "2              2   researchers say we have three years to act on...   698562\n",
       "3              1   wired  2016 was a pivotal year in the war on ...   573736\n",
       "4              1   its 2016 and a racist sexist climate change d...   466954\n",
       "...          ...                                                ...      ...\n",
       "15814          1   they took down the material on global warming...    22001\n",
       "15815          2   how climate change could be breaking up a 200...    17856\n",
       "15816          0  notiven rt nytimesworld what does trump actual...   384248\n",
       "15817         -1   hey liberals the climate change crap is a hoa...   819732\n",
       "15818          0   s climate change equation in 4 screenshots ur...   806319\n",
       "\n",
       "[15819 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing tokenizing library\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#importing stemmer library\n",
    "from nltk import SnowballStemmer\n",
    "#importing stemmer library\n",
    "from nltk import SnowballStemmer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Pre processing the data by creating new columns each with feature\n",
    "normalization technique applied, the use of outer functions also help\n",
    "in cleaning and removing the stop words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# lemmatizing function\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def tweet_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words if word.isalpha()]    \n",
    "\n",
    "#Stemmer function\n",
    "stemmer = SnowballStemmer('english')\n",
    "def token_stemmer(words, stemmer):\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def remove_stop_words(tokens):    \n",
    "    return [t for t in tokens if t not in stopwords.words('english')]\n",
    "\n",
    "def token_lemmatizer_stemmer(df):\n",
    "    \n",
    "    #tokenise the tweets and create a column\n",
    "    tokeniser = TreebankWordTokenizer()\n",
    "    df['tokens'] = df['message'].apply(tokeniser.tokenize)\n",
    "     \n",
    "    #creating a lemma column   \n",
    "    df['lemma'] = df['tokens'].apply(tweet_lemma, args=(lemmatizer, ))\n",
    "    \n",
    "    # find the stem of each word in the original tokens\n",
    "    df['original_stem'] = df['tokens'].apply(token_stemmer, args=(stemmer, ))\n",
    "    \n",
    "    # find the stem of each word in the Lemma tokens\n",
    "    df['lemma_stem'] = df['lemma'].apply(token_stemmer, args=(stemmer, ))\n",
    "    \n",
    "    #removing the stop words\n",
    "    df['lemma_no_stop_words'] = df['lemma_stem'].apply(remove_stop_words)\n",
    "    \n",
    "    #making the original stemmer\n",
    "    df['original_no_stop_words'] = df['lemma_stem'].apply(remove_stop_words)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "      <th>original_stem</th>\n",
       "      <th>lemma_stem</th>\n",
       "      <th>lemma_no_stop_words</th>\n",
       "      <th>original_no_stop_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>its not like we lack evidence of anthropogenic...</td>\n",
       "      <td>126103</td>\n",
       "      <td>[its, not, like, we, lack, evidence, of, anthr...</td>\n",
       "      <td>[it, not, like, we, lack, evidence, of, anthro...</td>\n",
       "      <td>[it, not, like, we, lack, evid, of, anthropoge...</td>\n",
       "      <td>[it, not, like, we, lack, evid, of, anthropoge...</td>\n",
       "      <td>[like, lack, evid, anthropogen, global, warm]</td>\n",
       "      <td>[like, lack, evid, anthropogen, global, warm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>researchers say we have three years to act on...</td>\n",
       "      <td>698562</td>\n",
       "      <td>[researchers, say, we, have, three, years, to,...</td>\n",
       "      <td>[researcher, say, we, have, three, year, to, a...</td>\n",
       "      <td>[research, say, we, have, three, year, to, act...</td>\n",
       "      <td>[research, say, we, have, three, year, to, act...</td>\n",
       "      <td>[research, say, three, year, act, climat, chan...</td>\n",
       "      <td>[research, say, three, year, act, climat, chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>wired  2016 was a pivotal year in the war on ...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[wired, 2016, was, a, pivotal, year, in, the, ...</td>\n",
       "      <td>[wired, wa, a, pivotal, year, in, the, war, on...</td>\n",
       "      <td>[wire, 2016, was, a, pivot, year, in, the, war...</td>\n",
       "      <td>[wire, wa, a, pivot, year, in, the, war, on, c...</td>\n",
       "      <td>[wire, wa, pivot, year, war, climat, chang, ur...</td>\n",
       "      <td>[wire, wa, pivot, year, war, climat, chang, ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>its 2016 and a racist sexist climate change d...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[its, 2016, and, a, racist, sexist, climate, c...</td>\n",
       "      <td>[it, and, a, racist, sexist, climate, change, ...</td>\n",
       "      <td>[it, 2016, and, a, racist, sexist, climat, cha...</td>\n",
       "      <td>[it, and, a, racist, sexist, climat, chang, de...</td>\n",
       "      <td>[racist, sexist, climat, chang, deni, bigot, l...</td>\n",
       "      <td>[racist, sexist, climat, chang, deni, bigot, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  polyscimajor epa chief doesnt think carbon dio...   625221   \n",
       "1          1  its not like we lack evidence of anthropogenic...   126103   \n",
       "2          2   researchers say we have three years to act on...   698562   \n",
       "3          1   wired  2016 was a pivotal year in the war on ...   573736   \n",
       "4          1   its 2016 and a racist sexist climate change d...   466954   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [its, not, like, we, lack, evidence, of, anthr...   \n",
       "2  [researchers, say, we, have, three, years, to,...   \n",
       "3  [wired, 2016, was, a, pivotal, year, in, the, ...   \n",
       "4  [its, 2016, and, a, racist, sexist, climate, c...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [it, not, like, we, lack, evidence, of, anthro...   \n",
       "2  [researcher, say, we, have, three, year, to, a...   \n",
       "3  [wired, wa, a, pivotal, year, in, the, war, on...   \n",
       "4  [it, and, a, racist, sexist, climate, change, ...   \n",
       "\n",
       "                                       original_stem  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [it, not, like, we, lack, evid, of, anthropoge...   \n",
       "2  [research, say, we, have, three, year, to, act...   \n",
       "3  [wire, 2016, was, a, pivot, year, in, the, war...   \n",
       "4  [it, 2016, and, a, racist, sexist, climat, cha...   \n",
       "\n",
       "                                          lemma_stem  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [it, not, like, we, lack, evid, of, anthropoge...   \n",
       "2  [research, say, we, have, three, year, to, act...   \n",
       "3  [wire, wa, a, pivot, year, in, the, war, on, c...   \n",
       "4  [it, and, a, racist, sexist, climat, chang, de...   \n",
       "\n",
       "                                 lemma_no_stop_words  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1      [like, lack, evid, anthropogen, global, warm]   \n",
       "2  [research, say, three, year, act, climat, chan...   \n",
       "3  [wire, wa, pivot, year, war, climat, chang, ur...   \n",
       "4  [racist, sexist, climat, chang, deni, bigot, l...   \n",
       "\n",
       "                              original_no_stop_words  \n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...  \n",
       "1      [like, lack, evid, anthropogen, global, warm]  \n",
       "2  [research, say, three, year, act, climat, chan...  \n",
       "3  [wire, wa, pivot, year, war, climat, chang, ur...  \n",
       "4  [racist, sexist, climat, chang, deni, bigot, l...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = token_lemmatizer_stemmer(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a class size of roughly half the size of the largest size\n",
    "class_size = 5000\n",
    "\n",
    "# Downsample classes with more than 5000 observations\n",
    "pro_downsampled = resample(df[df['sentiment']==1],\n",
    "                          replace=False, # sample without replacement (no need to duplicate observations)\n",
    "                          n_samples=class_size, # match number in class_size\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# Upsample classes with less than 5000 observations\n",
    "neutral_upsampled = resample(df[df['sentiment']==0],\n",
    "                          replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                          n_samples=class_size, # match number in class_size\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# Upsample classes with less than 5000 observations\n",
    "anti_upsampled = resample(df[df['sentiment']==-1],\n",
    "                          replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                          n_samples=class_size, # match number in class_size\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# Upsample classes with less than 5000 observations\n",
    "news_upsampled = resample(df[df['sentiment']==2],\n",
    "                          replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                          n_samples=class_size, # match number in class_size\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    5000\n",
       " 0    5000\n",
       "-1    5000\n",
       " 2    5000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine sampled classes with majority class\n",
    "sampled = pd.concat([pro_downsampled, neutral_upsampled, anti_upsampled, news_upsampled])\n",
    "\n",
    "# Check new class counts\n",
    "sampled['sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "      <th>original_stem</th>\n",
       "      <th>lemma_stem</th>\n",
       "      <th>lemma_no_stop_words</th>\n",
       "      <th>original_no_stop_words</th>\n",
       "      <th>token_no_stop_word</th>\n",
       "      <th>stem_no_stop_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11729</th>\n",
       "      <td>1</td>\n",
       "      <td>funding from  will support s team as they add...</td>\n",
       "      <td>977844</td>\n",
       "      <td>[funding, from, will, support, s, team, as, th...</td>\n",
       "      <td>[funding, from, will, support, s, team, a, the...</td>\n",
       "      <td>[fund, from, will, support, s, team, as, they,...</td>\n",
       "      <td>[fund, from, will, support, s, team, a, they, ...</td>\n",
       "      <td>[fund, support, team, address, impact, climat,...</td>\n",
       "      <td>[fund, support, team, address, impact, climat,...</td>\n",
       "      <td>[funding, support, team, address, impact, clim...</td>\n",
       "      <td>[fund, support, team, address, impact, climat,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8308</th>\n",
       "      <td>1</td>\n",
       "      <td>gag orders sure hes definitely green and does...</td>\n",
       "      <td>441956</td>\n",
       "      <td>[gag, orders, sure, hes, definitely, green, an...</td>\n",
       "      <td>[gag, order, sure, he, definitely, green, and,...</td>\n",
       "      <td>[gag, order, sure, hes, definit, green, and, d...</td>\n",
       "      <td>[gag, order, sure, he, definit, green, and, do...</td>\n",
       "      <td>[gag, order, sure, definit, green, doesnt, thi...</td>\n",
       "      <td>[gag, order, sure, definit, green, doesnt, thi...</td>\n",
       "      <td>[gag, orders, sure, hes, definitely, green, do...</td>\n",
       "      <td>[gag, order, sure, hes, definit, green, doesnt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7159</th>\n",
       "      <td>1</td>\n",
       "      <td>not ominous at all he also wants the names of...</td>\n",
       "      <td>978938</td>\n",
       "      <td>[not, ominous, at, all, he, also, wants, the, ...</td>\n",
       "      <td>[not, ominous, at, all, he, also, want, the, n...</td>\n",
       "      <td>[not, omin, at, all, he, also, want, the, name...</td>\n",
       "      <td>[not, omin, at, all, he, also, want, the, name...</td>\n",
       "      <td>[omin, also, want, name, anyon, work, climat, ...</td>\n",
       "      <td>[omin, also, want, name, anyon, work, climat, ...</td>\n",
       "      <td>[ominous, also, wants, names, anyone, working,...</td>\n",
       "      <td>[omin, also, want, name, anyon, work, climat, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5644</th>\n",
       "      <td>1</td>\n",
       "      <td>in case you forgot about that chinese hoax gl...</td>\n",
       "      <td>587737</td>\n",
       "      <td>[in, case, you, forgot, about, that, chinese, ...</td>\n",
       "      <td>[in, case, you, forgot, about, that, chinese, ...</td>\n",
       "      <td>[in, case, you, forgot, about, that, chines, h...</td>\n",
       "      <td>[in, case, you, forgot, about, that, chines, h...</td>\n",
       "      <td>[case, forgot, chines, hoax, global, warm, url...</td>\n",
       "      <td>[case, forgot, chines, hoax, global, warm, url...</td>\n",
       "      <td>[case, forgot, chinese, hoax, global, warming,...</td>\n",
       "      <td>[case, forgot, chines, hoax, global, warm, url...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6732</th>\n",
       "      <td>1</td>\n",
       "      <td>hrc proposes installing half a billion solar ...</td>\n",
       "      <td>804767</td>\n",
       "      <td>[hrc, proposes, installing, half, a, billion, ...</td>\n",
       "      <td>[hrc, proposes, installing, half, a, billion, ...</td>\n",
       "      <td>[hrc, propos, instal, half, a, billion, solar,...</td>\n",
       "      <td>[hrc, propos, instal, half, a, billion, solar,...</td>\n",
       "      <td>[hrc, propos, instal, half, billion, solar, pa...</td>\n",
       "      <td>[hrc, propos, instal, half, billion, solar, pa...</td>\n",
       "      <td>[hrc, proposes, installing, half, billion, sol...</td>\n",
       "      <td>[hrc, propos, instal, half, billion, solar, pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            message  tweetid  \\\n",
       "11729          1   funding from  will support s team as they add...   977844   \n",
       "8308           1   gag orders sure hes definitely green and does...   441956   \n",
       "7159           1   not ominous at all he also wants the names of...   978938   \n",
       "5644           1   in case you forgot about that chinese hoax gl...   587737   \n",
       "6732           1   hrc proposes installing half a billion solar ...   804767   \n",
       "\n",
       "                                                  tokens  \\\n",
       "11729  [funding, from, will, support, s, team, as, th...   \n",
       "8308   [gag, orders, sure, hes, definitely, green, an...   \n",
       "7159   [not, ominous, at, all, he, also, wants, the, ...   \n",
       "5644   [in, case, you, forgot, about, that, chinese, ...   \n",
       "6732   [hrc, proposes, installing, half, a, billion, ...   \n",
       "\n",
       "                                                   lemma  \\\n",
       "11729  [funding, from, will, support, s, team, a, the...   \n",
       "8308   [gag, order, sure, he, definitely, green, and,...   \n",
       "7159   [not, ominous, at, all, he, also, want, the, n...   \n",
       "5644   [in, case, you, forgot, about, that, chinese, ...   \n",
       "6732   [hrc, proposes, installing, half, a, billion, ...   \n",
       "\n",
       "                                           original_stem  \\\n",
       "11729  [fund, from, will, support, s, team, as, they,...   \n",
       "8308   [gag, order, sure, hes, definit, green, and, d...   \n",
       "7159   [not, omin, at, all, he, also, want, the, name...   \n",
       "5644   [in, case, you, forgot, about, that, chines, h...   \n",
       "6732   [hrc, propos, instal, half, a, billion, solar,...   \n",
       "\n",
       "                                              lemma_stem  \\\n",
       "11729  [fund, from, will, support, s, team, a, they, ...   \n",
       "8308   [gag, order, sure, he, definit, green, and, do...   \n",
       "7159   [not, omin, at, all, he, also, want, the, name...   \n",
       "5644   [in, case, you, forgot, about, that, chines, h...   \n",
       "6732   [hrc, propos, instal, half, a, billion, solar,...   \n",
       "\n",
       "                                     lemma_no_stop_words  \\\n",
       "11729  [fund, support, team, address, impact, climat,...   \n",
       "8308   [gag, order, sure, definit, green, doesnt, thi...   \n",
       "7159   [omin, also, want, name, anyon, work, climat, ...   \n",
       "5644   [case, forgot, chines, hoax, global, warm, url...   \n",
       "6732   [hrc, propos, instal, half, billion, solar, pa...   \n",
       "\n",
       "                                  original_no_stop_words  \\\n",
       "11729  [fund, support, team, address, impact, climat,...   \n",
       "8308   [gag, order, sure, definit, green, doesnt, thi...   \n",
       "7159   [omin, also, want, name, anyon, work, climat, ...   \n",
       "5644   [case, forgot, chines, hoax, global, warm, url...   \n",
       "6732   [hrc, propos, instal, half, billion, solar, pa...   \n",
       "\n",
       "                                      token_no_stop_word  \\\n",
       "11729  [funding, support, team, address, impact, clim...   \n",
       "8308   [gag, orders, sure, hes, definitely, green, do...   \n",
       "7159   [ominous, also, wants, names, anyone, working,...   \n",
       "5644   [case, forgot, chinese, hoax, global, warming,...   \n",
       "6732   [hrc, proposes, installing, half, billion, sol...   \n",
       "\n",
       "                                       stem_no_stop_word  \n",
       "11729  [fund, support, team, address, impact, climat,...  \n",
       "8308   [gag, order, sure, hes, definit, green, doesnt...  \n",
       "7159   [omin, also, want, name, anyon, work, climat, ...  \n",
       "5644   [case, forgot, chines, hoax, global, warm, url...  \n",
       "6732   [hrc, propos, instal, half, billion, solar, pa...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words first\n",
    "sampled['token_no_stop_word'] = sampled['tokens'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words first\n",
    "sampled['stem_no_stop_word'] = sampled['token_no_stop_word'].apply(token_stemmer, args=(stemmer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences = [\" \".join(i) for i in sampled['stem_no_stop_word']]\n",
    "sampled['clean_sentences'] = clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "      <th>original_stem</th>\n",
       "      <th>lemma_stem</th>\n",
       "      <th>lemma_no_stop_words</th>\n",
       "      <th>original_no_stop_words</th>\n",
       "      <th>token_no_stop_word</th>\n",
       "      <th>stem_no_stop_word</th>\n",
       "      <th>clean_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11729</th>\n",
       "      <td>1</td>\n",
       "      <td>funding from  will support s team as they add...</td>\n",
       "      <td>977844</td>\n",
       "      <td>[funding, from, will, support, s, team, as, th...</td>\n",
       "      <td>[funding, from, will, support, s, team, a, the...</td>\n",
       "      <td>[fund, from, will, support, s, team, as, they,...</td>\n",
       "      <td>[fund, from, will, support, s, team, a, they, ...</td>\n",
       "      <td>[fund, support, team, address, impact, climat,...</td>\n",
       "      <td>[fund, support, team, address, impact, climat,...</td>\n",
       "      <td>[funding, support, team, address, impact, clim...</td>\n",
       "      <td>[fund, support, team, address, impact, climat,...</td>\n",
       "      <td>fund support team address impact climat chang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8308</th>\n",
       "      <td>1</td>\n",
       "      <td>gag orders sure hes definitely green and does...</td>\n",
       "      <td>441956</td>\n",
       "      <td>[gag, orders, sure, hes, definitely, green, an...</td>\n",
       "      <td>[gag, order, sure, he, definitely, green, and,...</td>\n",
       "      <td>[gag, order, sure, hes, definit, green, and, d...</td>\n",
       "      <td>[gag, order, sure, he, definit, green, and, do...</td>\n",
       "      <td>[gag, order, sure, definit, green, doesnt, thi...</td>\n",
       "      <td>[gag, order, sure, definit, green, doesnt, thi...</td>\n",
       "      <td>[gag, orders, sure, hes, definitely, green, do...</td>\n",
       "      <td>[gag, order, sure, hes, definit, green, doesnt...</td>\n",
       "      <td>gag order sure hes definit green doesnt think ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7159</th>\n",
       "      <td>1</td>\n",
       "      <td>not ominous at all he also wants the names of...</td>\n",
       "      <td>978938</td>\n",
       "      <td>[not, ominous, at, all, he, also, wants, the, ...</td>\n",
       "      <td>[not, ominous, at, all, he, also, want, the, n...</td>\n",
       "      <td>[not, omin, at, all, he, also, want, the, name...</td>\n",
       "      <td>[not, omin, at, all, he, also, want, the, name...</td>\n",
       "      <td>[omin, also, want, name, anyon, work, climat, ...</td>\n",
       "      <td>[omin, also, want, name, anyon, work, climat, ...</td>\n",
       "      <td>[ominous, also, wants, names, anyone, working,...</td>\n",
       "      <td>[omin, also, want, name, anyon, work, climat, ...</td>\n",
       "      <td>omin also want name anyon work climat chang re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5644</th>\n",
       "      <td>1</td>\n",
       "      <td>in case you forgot about that chinese hoax gl...</td>\n",
       "      <td>587737</td>\n",
       "      <td>[in, case, you, forgot, about, that, chinese, ...</td>\n",
       "      <td>[in, case, you, forgot, about, that, chinese, ...</td>\n",
       "      <td>[in, case, you, forgot, about, that, chines, h...</td>\n",
       "      <td>[in, case, you, forgot, about, that, chines, h...</td>\n",
       "      <td>[case, forgot, chines, hoax, global, warm, url...</td>\n",
       "      <td>[case, forgot, chines, hoax, global, warm, url...</td>\n",
       "      <td>[case, forgot, chinese, hoax, global, warming,...</td>\n",
       "      <td>[case, forgot, chines, hoax, global, warm, url...</td>\n",
       "      <td>case forgot chines hoax global warm urlweb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6732</th>\n",
       "      <td>1</td>\n",
       "      <td>hrc proposes installing half a billion solar ...</td>\n",
       "      <td>804767</td>\n",
       "      <td>[hrc, proposes, installing, half, a, billion, ...</td>\n",
       "      <td>[hrc, proposes, installing, half, a, billion, ...</td>\n",
       "      <td>[hrc, propos, instal, half, a, billion, solar,...</td>\n",
       "      <td>[hrc, propos, instal, half, a, billion, solar,...</td>\n",
       "      <td>[hrc, propos, instal, half, billion, solar, pa...</td>\n",
       "      <td>[hrc, propos, instal, half, billion, solar, pa...</td>\n",
       "      <td>[hrc, proposes, installing, half, billion, sol...</td>\n",
       "      <td>[hrc, propos, instal, half, billion, solar, pa...</td>\n",
       "      <td>hrc propos instal half billion solar panel end...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            message  tweetid  \\\n",
       "11729          1   funding from  will support s team as they add...   977844   \n",
       "8308           1   gag orders sure hes definitely green and does...   441956   \n",
       "7159           1   not ominous at all he also wants the names of...   978938   \n",
       "5644           1   in case you forgot about that chinese hoax gl...   587737   \n",
       "6732           1   hrc proposes installing half a billion solar ...   804767   \n",
       "\n",
       "                                                  tokens  \\\n",
       "11729  [funding, from, will, support, s, team, as, th...   \n",
       "8308   [gag, orders, sure, hes, definitely, green, an...   \n",
       "7159   [not, ominous, at, all, he, also, wants, the, ...   \n",
       "5644   [in, case, you, forgot, about, that, chinese, ...   \n",
       "6732   [hrc, proposes, installing, half, a, billion, ...   \n",
       "\n",
       "                                                   lemma  \\\n",
       "11729  [funding, from, will, support, s, team, a, the...   \n",
       "8308   [gag, order, sure, he, definitely, green, and,...   \n",
       "7159   [not, ominous, at, all, he, also, want, the, n...   \n",
       "5644   [in, case, you, forgot, about, that, chinese, ...   \n",
       "6732   [hrc, proposes, installing, half, a, billion, ...   \n",
       "\n",
       "                                           original_stem  \\\n",
       "11729  [fund, from, will, support, s, team, as, they,...   \n",
       "8308   [gag, order, sure, hes, definit, green, and, d...   \n",
       "7159   [not, omin, at, all, he, also, want, the, name...   \n",
       "5644   [in, case, you, forgot, about, that, chines, h...   \n",
       "6732   [hrc, propos, instal, half, a, billion, solar,...   \n",
       "\n",
       "                                              lemma_stem  \\\n",
       "11729  [fund, from, will, support, s, team, a, they, ...   \n",
       "8308   [gag, order, sure, he, definit, green, and, do...   \n",
       "7159   [not, omin, at, all, he, also, want, the, name...   \n",
       "5644   [in, case, you, forgot, about, that, chines, h...   \n",
       "6732   [hrc, propos, instal, half, a, billion, solar,...   \n",
       "\n",
       "                                     lemma_no_stop_words  \\\n",
       "11729  [fund, support, team, address, impact, climat,...   \n",
       "8308   [gag, order, sure, definit, green, doesnt, thi...   \n",
       "7159   [omin, also, want, name, anyon, work, climat, ...   \n",
       "5644   [case, forgot, chines, hoax, global, warm, url...   \n",
       "6732   [hrc, propos, instal, half, billion, solar, pa...   \n",
       "\n",
       "                                  original_no_stop_words  \\\n",
       "11729  [fund, support, team, address, impact, climat,...   \n",
       "8308   [gag, order, sure, definit, green, doesnt, thi...   \n",
       "7159   [omin, also, want, name, anyon, work, climat, ...   \n",
       "5644   [case, forgot, chines, hoax, global, warm, url...   \n",
       "6732   [hrc, propos, instal, half, billion, solar, pa...   \n",
       "\n",
       "                                      token_no_stop_word  \\\n",
       "11729  [funding, support, team, address, impact, clim...   \n",
       "8308   [gag, orders, sure, hes, definitely, green, do...   \n",
       "7159   [ominous, also, wants, names, anyone, working,...   \n",
       "5644   [case, forgot, chinese, hoax, global, warming,...   \n",
       "6732   [hrc, proposes, installing, half, billion, sol...   \n",
       "\n",
       "                                       stem_no_stop_word  \\\n",
       "11729  [fund, support, team, address, impact, climat,...   \n",
       "8308   [gag, order, sure, hes, definit, green, doesnt...   \n",
       "7159   [omin, also, want, name, anyon, work, climat, ...   \n",
       "5644   [case, forgot, chines, hoax, global, warm, url...   \n",
       "6732   [hrc, propos, instal, half, billion, solar, pa...   \n",
       "\n",
       "                                         clean_sentences  \n",
       "11729  fund support team address impact climat chang ...  \n",
       "8308   gag order sure hes definit green doesnt think ...  \n",
       "7159   omin also want name anyon work climat chang re...  \n",
       "5644          case forgot chines hoax global warm urlweb  \n",
       "6732   hrc propos instal half billion solar panel end...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "# X -> features, y -> label\n",
    "y =  sampled['sentiment']\n",
    "X =  sampled['clean_sentences']\n",
    "\n",
    "# dividing X, y into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                random_state = 42)\n",
    "\n",
    "# extracting features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "V_train_X = vectorizer.fit_transform(X_train)\n",
    "V_test_X = vectorizer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.39      0.17      0.24      1256\n",
      "           0       0.47      0.36      0.41      1280\n",
      "           1       0.36      0.52      0.43      1244\n",
      "           2       0.52      0.70      0.60      1220\n",
      "\n",
      "    accuracy                           0.44      5000\n",
      "   macro avg       0.43      0.44      0.42      5000\n",
      "weighted avg       0.43      0.44      0.41      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    " # training a DescisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree_model = DecisionTreeClassifier(max_depth = 2).fit(V_train_X, y_train)\n",
    "dtree_predictions = dtree_model.predict(V_test_X)\n",
    " \n",
    "# creating a confusion matrix\n",
    "cm1 = confusion_matrix(y_test, dtree_predictions)\n",
    "print('Classification Report Decision Tree')\n",
    "print(classification_report(y_test, dtree_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.86      0.91      0.89      1256\n",
      "           0       0.80      0.81      0.80      1280\n",
      "           1       0.73      0.65      0.69      1244\n",
      "           2       0.83      0.87      0.85      1220\n",
      "\n",
      "    accuracy                           0.81      5000\n",
      "   macro avg       0.81      0.81      0.81      5000\n",
      "weighted avg       0.81      0.81      0.81      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training a linear SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(V_train_X, y_train)\n",
    "svm_predictions = svm_model_linear.predict(V_test_X)\n",
    " \n",
    "# model accuracy for V_test_X \n",
    "accuracy = svm_model_linear.score(V_test_X, y_test)\n",
    " \n",
    "# creating a confusion matrix\n",
    "cm2 = confusion_matrix(y_test, svm_predictions)\n",
    "\n",
    "print('Classification Report SVM')\n",
    "print(classification_report(y_test, svm_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1149,   55,   42,   10],\n",
       "       [  66, 1035,  135,   44],\n",
       "       [ 102,  180,  803,  159],\n",
       "       [  14,   26,  116, 1064]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6552\n",
      "Classification Report KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.80      0.73      1256\n",
      "           0       0.59      0.66      0.62      1280\n",
      "           1       0.65      0.39      0.48      1244\n",
      "           2       0.72      0.77      0.74      1220\n",
      "\n",
      "    accuracy                           0.66      5000\n",
      "   macro avg       0.66      0.66      0.64      5000\n",
      "weighted avg       0.65      0.66      0.64      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training a KNN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 7).fit(V_train_X, y_train)\n",
    " \n",
    "# accuracy on V_test_X\n",
    "accuracy = knn.score(V_test_X, y_test)\n",
    "print(accuracy)\n",
    " \n",
    "# creating a confusion matrix\n",
    "knn_predictions = knn.predict(V_test_X)\n",
    "cm3 = confusion_matrix(y_test, knn_predictions)\n",
    "\n",
    "print('Classification Report KNN')\n",
    "print(classification_report(y_test, knn_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6662\n",
      "Classification Report Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.98      0.72      1256\n",
      "           0       0.80      0.64      0.71      1280\n",
      "           1       0.75      0.29      0.42      1244\n",
      "           2       0.70      0.75      0.72      1220\n",
      "\n",
      "    accuracy                           0.67      5000\n",
      "   macro avg       0.70      0.67      0.64      5000\n",
      "weighted avg       0.70      0.67      0.64      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training a Naive classifierBayes \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB().fit(V_train_X.toarray(), y_train)\n",
    "gnb_predictions = gnb.predict(V_test_X.toarray())\n",
    " \n",
    "# accuracy on V_test_X\n",
    "accuracy = gnb.score(V_test_X.toarray(), y_test)\n",
    "print(accuracy)\n",
    " \n",
    "# creating a confusion matrix\n",
    "cm4 = confusion_matrix(y_test, gnb_predictions)\n",
    "\n",
    "\n",
    "print('Classification Report Naive Bayes')\n",
    "print(classification_report(y_test, gnb_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training a Naive classifierBayes \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lm_vt = LogisticRegression(multi_class='ovr').fit(V_train_X, y_train)\n",
    "pred_lr_vt = lm_vt.predict(V_test_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy on V_test_X\n",
    "accuracy = gnb.score(pred_lr_vt, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report Linear logistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.88      0.86      1256\n",
      "           0       0.77      0.77      0.77      1280\n",
      "           1       0.72      0.61      0.66      1244\n",
      "           2       0.79      0.88      0.83      1220\n",
      "\n",
      "    accuracy                           0.78      5000\n",
      "   macro avg       0.78      0.78      0.78      5000\n",
      "weighted avg       0.78      0.78      0.78      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report Linear logistics')\n",
    "print(classification_report(y_test, pred_lr_vt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making a Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traing the model with the entire data\n",
    "yf = sampled['sentiment']\n",
    "Xf = sampled['clean_sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test = data_cleaner(df_test, 'message')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>europe will now be looking to china to make su...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the scary unimpeachable evidence that climate ...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nputin got to you too jill  \\ntrump doesn...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female orgasms cause global warming\\nsarcasti...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10541</th>\n",
       "      <td>brb writing a poem about climate change      ...</td>\n",
       "      <td>895714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10542</th>\n",
       "      <td>2016 the year climate change came home during ...</td>\n",
       "      <td>875167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10543</th>\n",
       "      <td>pacific countries positive about fiji leading...</td>\n",
       "      <td>78329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10544</th>\n",
       "      <td>you’re so hot you must be the cause for globa...</td>\n",
       "      <td>867455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10545</th>\n",
       "      <td>climate change is a global issue thats only g...</td>\n",
       "      <td>470892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 message  tweetid\n",
       "0      europe will now be looking to china to make su...   169760\n",
       "1      combine this with the polling of staffers re c...    35326\n",
       "2      the scary unimpeachable evidence that climate ...   224985\n",
       "3          \\nputin got to you too jill  \\ntrump doesn...   476263\n",
       "4       female orgasms cause global warming\\nsarcasti...   872928\n",
       "...                                                  ...      ...\n",
       "10541   brb writing a poem about climate change      ...   895714\n",
       "10542  2016 the year climate change came home during ...   875167\n",
       "10543   pacific countries positive about fiji leading...    78329\n",
       "10544   you’re so hot you must be the cause for globa...   867455\n",
       "10545   climate change is a global issue thats only g...   470892\n",
       "\n",
       "[10546 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenise the tweets and create a column\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "clean_test['tokens'] = clean_test['message'].apply(tokeniser.tokenize)\n",
    "\n",
    "# Remove stop words first\n",
    "clean_test['token_no_stop_word'] = clean_test['tokens'].apply(remove_stop_words)\n",
    "\n",
    "# Remove stop words first\n",
    "clean_test['stem_no_stop_word'] = clean_test['token_no_stop_word'].apply(token_stemmer, args=(stemmer, ))\n",
    "\n",
    "clean_sentences = [\" \".join(i) for i in clean_test['stem_no_stop_word']]\n",
    "clean_test['clean_sentences'] = clean_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_no_stop_word</th>\n",
       "      <th>stem_no_stop_word</th>\n",
       "      <th>clean_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>europe will now be looking to china to make su...</td>\n",
       "      <td>169760</td>\n",
       "      <td>[europe, will, now, be, looking, to, china, to...</td>\n",
       "      <td>[europe, looking, china, make, sure, alone, fi...</td>\n",
       "      <td>[europ, look, china, make, sure, alon, fight, ...</td>\n",
       "      <td>europ look china make sure alon fight climat c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "      <td>[combine, this, with, the, polling, of, staffe...</td>\n",
       "      <td>[combine, polling, staffers, climate, change, ...</td>\n",
       "      <td>[combin, poll, staffer, climat, chang, women, ...</td>\n",
       "      <td>combin poll staffer climat chang women right f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the scary unimpeachable evidence that climate ...</td>\n",
       "      <td>224985</td>\n",
       "      <td>[the, scary, unimpeachable, evidence, that, cl...</td>\n",
       "      <td>[scary, unimpeachable, evidence, climate, chan...</td>\n",
       "      <td>[scari, unimpeach, evid, climat, chang, alread...</td>\n",
       "      <td>scari unimpeach evid climat chang alreadi urlweb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nputin got to you too jill  \\ntrump doesn...</td>\n",
       "      <td>476263</td>\n",
       "      <td>[putin, got, to, you, too, jill, trump, doesnt...</td>\n",
       "      <td>[putin, got, jill, trump, doesnt, believe, cli...</td>\n",
       "      <td>[putin, got, jill, trump, doesnt, believ, clim...</td>\n",
       "      <td>putin got jill trump doesnt believ climat chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female orgasms cause global warming\\nsarcasti...</td>\n",
       "      <td>872928</td>\n",
       "      <td>[female, orgasms, cause, global, warming, sarc...</td>\n",
       "      <td>[female, orgasms, cause, global, warming, sarc...</td>\n",
       "      <td>[femal, orgasm, caus, global, warm, sarcast, r...</td>\n",
       "      <td>femal orgasm caus global warm sarcast republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10541</th>\n",
       "      <td>brb writing a poem about climate change      ...</td>\n",
       "      <td>895714</td>\n",
       "      <td>[brb, writing, a, poem, about, climate, change...</td>\n",
       "      <td>[brb, writing, poem, climate, change, urlweb…]</td>\n",
       "      <td>[brb, write, poem, climat, chang, urlweb…]</td>\n",
       "      <td>brb write poem climat chang urlweb…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10542</th>\n",
       "      <td>2016 the year climate change came home during ...</td>\n",
       "      <td>875167</td>\n",
       "      <td>[2016, the, year, climate, change, came, home,...</td>\n",
       "      <td>[2016, year, climate, change, came, home, hott...</td>\n",
       "      <td>[2016, year, climat, chang, came, home, hottes...</td>\n",
       "      <td>2016 year climat chang came home hottest year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10543</th>\n",
       "      <td>pacific countries positive about fiji leading...</td>\n",
       "      <td>78329</td>\n",
       "      <td>[pacific, countries, positive, about, fiji, le...</td>\n",
       "      <td>[pacific, countries, positive, fiji, leading, ...</td>\n",
       "      <td>[pacif, countri, posit, fiji, lead, global, cl...</td>\n",
       "      <td>pacif countri posit fiji lead global climat ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10544</th>\n",
       "      <td>you’re so hot you must be the cause for globa...</td>\n",
       "      <td>867455</td>\n",
       "      <td>[you’re, so, hot, you, must, be, the, cause, f...</td>\n",
       "      <td>[you’re, hot, must, cause, global, warming]</td>\n",
       "      <td>[you'r, hot, must, caus, global, warm]</td>\n",
       "      <td>you'r hot must caus global warm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10545</th>\n",
       "      <td>climate change is a global issue thats only g...</td>\n",
       "      <td>470892</td>\n",
       "      <td>[climate, change, is, a, global, issue, thats,...</td>\n",
       "      <td>[climate, change, global, issue, thats, gettin...</td>\n",
       "      <td>[climat, chang, global, issu, that, get, wors,...</td>\n",
       "      <td>climat chang global issu that get wors eat pla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10546 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 message  tweetid  \\\n",
       "0      europe will now be looking to china to make su...   169760   \n",
       "1      combine this with the polling of staffers re c...    35326   \n",
       "2      the scary unimpeachable evidence that climate ...   224985   \n",
       "3          \\nputin got to you too jill  \\ntrump doesn...   476263   \n",
       "4       female orgasms cause global warming\\nsarcasti...   872928   \n",
       "...                                                  ...      ...   \n",
       "10541   brb writing a poem about climate change      ...   895714   \n",
       "10542  2016 the year climate change came home during ...   875167   \n",
       "10543   pacific countries positive about fiji leading...    78329   \n",
       "10544   you’re so hot you must be the cause for globa...   867455   \n",
       "10545   climate change is a global issue thats only g...   470892   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [europe, will, now, be, looking, to, china, to...   \n",
       "1      [combine, this, with, the, polling, of, staffe...   \n",
       "2      [the, scary, unimpeachable, evidence, that, cl...   \n",
       "3      [putin, got, to, you, too, jill, trump, doesnt...   \n",
       "4      [female, orgasms, cause, global, warming, sarc...   \n",
       "...                                                  ...   \n",
       "10541  [brb, writing, a, poem, about, climate, change...   \n",
       "10542  [2016, the, year, climate, change, came, home,...   \n",
       "10543  [pacific, countries, positive, about, fiji, le...   \n",
       "10544  [you’re, so, hot, you, must, be, the, cause, f...   \n",
       "10545  [climate, change, is, a, global, issue, thats,...   \n",
       "\n",
       "                                      token_no_stop_word  \\\n",
       "0      [europe, looking, china, make, sure, alone, fi...   \n",
       "1      [combine, polling, staffers, climate, change, ...   \n",
       "2      [scary, unimpeachable, evidence, climate, chan...   \n",
       "3      [putin, got, jill, trump, doesnt, believe, cli...   \n",
       "4      [female, orgasms, cause, global, warming, sarc...   \n",
       "...                                                  ...   \n",
       "10541     [brb, writing, poem, climate, change, urlweb…]   \n",
       "10542  [2016, year, climate, change, came, home, hott...   \n",
       "10543  [pacific, countries, positive, fiji, leading, ...   \n",
       "10544        [you’re, hot, must, cause, global, warming]   \n",
       "10545  [climate, change, global, issue, thats, gettin...   \n",
       "\n",
       "                                       stem_no_stop_word  \\\n",
       "0      [europ, look, china, make, sure, alon, fight, ...   \n",
       "1      [combin, poll, staffer, climat, chang, women, ...   \n",
       "2      [scari, unimpeach, evid, climat, chang, alread...   \n",
       "3      [putin, got, jill, trump, doesnt, believ, clim...   \n",
       "4      [femal, orgasm, caus, global, warm, sarcast, r...   \n",
       "...                                                  ...   \n",
       "10541         [brb, write, poem, climat, chang, urlweb…]   \n",
       "10542  [2016, year, climat, chang, came, home, hottes...   \n",
       "10543  [pacif, countri, posit, fiji, lead, global, cl...   \n",
       "10544             [you'r, hot, must, caus, global, warm]   \n",
       "10545  [climat, chang, global, issu, that, get, wors,...   \n",
       "\n",
       "                                         clean_sentences  \n",
       "0      europ look china make sure alon fight climat c...  \n",
       "1      combin poll staffer climat chang women right f...  \n",
       "2       scari unimpeach evid climat chang alreadi urlweb  \n",
       "3      putin got jill trump doesnt believ climat chan...  \n",
       "4       femal orgasm caus global warm sarcast republican  \n",
       "...                                                  ...  \n",
       "10541                brb write poem climat chang urlweb…  \n",
       "10542  2016 year climat chang came home hottest year ...  \n",
       "10543  pacif countri posit fiji lead global climat ch...  \n",
       "10544                    you'r hot must caus global warm  \n",
       "10545  climat chang global issu that get wors eat pla...  \n",
       "\n",
       "[10546 rows x 6 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testf = clean_test['clean_sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer1 = TfidfVectorizer()\n",
    "V_train_Xf = vectorizer1.fit_transform(Xf)\n",
    "V_test_Xf = vectorizer1.transform(X_testf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10546, 10417)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_test_Xf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a linear SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "svm_model_linear_f = SVC(kernel = 'linear', C = 1).fit(V_train_Xf,yf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = svm_model_linear_f.predict(V_test_Xf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "daf = pd.DataFrame(final_preds, columns=['sentiment'])\n",
    "daf.head()\n",
    "\n",
    "output = pd.DataFrame({\"tweetid\":df_test['tweetid']})\n",
    "final = output.join(daf)        \n",
    "final.to_csv(\"final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
