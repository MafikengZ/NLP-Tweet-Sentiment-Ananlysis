{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMx61ZPWczw7"
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6b_v8YFcHGe",
        "outputId": "570a301c-d87e-4581-8a83-43d0915ff265"
      },
      "source": [
        "!pip install comet_ml\n",
        "!pip install tweet-preprocessor\n",
        "!pip install nlppreprocess"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting comet_ml\n",
            "  Downloading comet_ml-3.23.0-py2.py3-none-any.whl (308 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 18.0 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61 kB 30.9 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 71 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 92 kB 25.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 143 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 184 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 215 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 235 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 256 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 276 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 286 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 296 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 308 kB 26.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (7.352.0)\n",
            "Collecting wurlitzer>=1.0.2\n",
            "  Downloading wurlitzer-3.0.2-py3-none-any.whl (7.3 kB)\n",
            "Collecting dulwich>=0.20.6\n",
            "  Downloading dulwich-0.20.26-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 44.1 MB/s \n",
            "\u001b[?25hCollecting requests-toolbelt>=0.8.0\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from comet_ml) (1.15.0)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (2.6.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (1.13.3)\n",
            "Collecting everett[ini]>=1.0.1\n",
            "  Downloading everett-2.0.1-py2.py3-none-any.whl (33 kB)\n",
            "Collecting semantic-version>=2.8.0\n",
            "  Downloading semantic_version-2.8.5-py2.py3-none-any.whl (15 kB)\n",
            "Collecting websocket-client>=0.55.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (2.23.0)\n",
            "Requirement already satisfied: urllib3>=1.24.1 in /usr/local/lib/python3.7/dist-packages (from dulwich>=0.20.6->comet_ml) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from dulwich>=0.20.6->comet_ml) (2021.10.8)\n",
            "Collecting configobj\n",
            "  Downloading configobj-5.0.6.tar.gz (33 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (2.10)\n",
            "Building wheels for collected packages: configobj\n",
            "  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.6-py3-none-any.whl size=34546 sha256=e2ecbcd91c7c4ba84bb1a5b3e799f02003b4cd024de84e84fa4d446cfe8a8f98\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/c4/19/13d74440f2a571841db6b6e0a273694327498884dafb9cf978\n",
            "Successfully built configobj\n",
            "Installing collected packages: everett, configobj, wurlitzer, websocket-client, semantic-version, requests-toolbelt, dulwich, comet-ml\n",
            "Successfully installed comet-ml-3.23.0 configobj-5.0.6 dulwich-0.20.26 everett-2.0.1 requests-toolbelt-0.9.1 semantic-version-2.8.5 websocket-client-1.2.1 wurlitzer-3.0.2\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n",
            "Collecting nlppreprocess\n",
            "  Downloading nlppreprocess-1.0.2-py3-none-any.whl (5.1 kB)\n",
            "Installing collected packages: nlppreprocess\n",
            "Successfully installed nlppreprocess-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSzPcpCJgsIM"
      },
      "source": [
        "#Data preprocessing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Data Visualization libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uEZMgR8f7J4"
      },
      "source": [
        "data = pd.read_csv('sample_data/train.csv', encoding='UTF-8')\n",
        "test = pd.read_csv('sample_data/test_with_no_labels.csv' ,encoding='UTF-8')\n",
        "\n",
        "#Insurance Dataset\n",
        "data_copy = data.copy()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6klsQHPgPhf"
      },
      "source": [
        "import preprocessor as p"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRRDxYPusJ4c"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc1GROIurth1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTPCVT2lg7yH"
      },
      "source": [
        "#Clean data using tweet preprocessor\n",
        "def _clean_tweet(data):\n",
        "  return p.clean(data)\n",
        "#Apply the function to the dataset\n",
        "data_copy['clean_tweets'] = data_copy['message'].apply(_clean_tweet)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALC7Q76O7_z5"
      },
      "source": [
        "import re\n",
        "import string"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0KEJ2hKhL48"
      },
      "source": [
        "def _remove_punctuation_numbers(data):\n",
        "   punc_numbers = string.punctuation + '0123456789'\n",
        "   return ''.join([l for l in data if l not in punc_numbers])\n",
        "data_copy['clean_punc'] = data_copy['clean_tweets'].apply(_remove_punctuation_numbers)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP0oCGKI6lNg"
      },
      "source": [
        "def _characters (data):\n",
        "  data = re.sub('\\\\n' , '' , data) #Remove any new lines\n",
        "  data = re.sub(r'[^\\x00-\\x7f]',r'', data)\n",
        "  return data\n",
        "data_copy['clean_char'] = data_copy['clean_punc'].apply(_characters)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6TWHr9D6Qat"
      },
      "source": [
        "def _lower(data):\n",
        "  return data.lower()\n",
        "data_copy['lower'] = data_copy['clean_punc'].apply(_lower)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jvg2xF07rrP",
        "outputId": "8b2c130a-2dee-4308-ed1b-2f9d9e14392a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopword = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RLxu7YqR9N3K",
        "outputId": "e8ba128d-9bfd-4811-892b-5dd4cb409af6"
      },
      "source": [
        "from nlppreprocess import NLP\n",
        "nlp = NLP()\n",
        "nlp.process('couldnt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'could not'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwsm6X0-81zu"
      },
      "source": [
        "#Remove Stopwords\n",
        "def remove_stopwords(data):\n",
        "    \"\"\"\n",
        "    Remove stop-word in the dataset to reduce noise\n",
        "    Args:\n",
        "        Args:\n",
        "        data: pandas dataframe\n",
        "    Return:\n",
        "        Dataframe:non-stop word\n",
        "    \"\"\"\n",
        "    stopwords = NLP(replace_words=True, remove_stopwords=True, \n",
        "                            remove_numbers=True, remove_punctuations=False) \n",
        "    data = stopwords.process(data)\n",
        "    return data\n",
        "    \n",
        "data_copy['Tweet_nonstop'] = data_copy['lower'].apply(lambda x: remove_stopwords(x))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6gpxZDV9Sph"
      },
      "source": [
        "#Tokenization\n",
        "def tokenization(data):\n",
        "    data = re.split('\\W+', data)\n",
        "    return data\n",
        "data_copy['Tweet_tokenized'] = data_copy['Tweet_nonstop'].apply(lambda x: tokenization(x))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VIXYEZ698Yu",
        "outputId": "1673dc4d-3dc7-42cc-922d-5dc37ae63ded"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "nltk.download('wordnet')\n",
        "lem = WordNetLemmatizer()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORBk9_mnullm",
        "outputId": "f8b17c3a-3e38-426c-fc20-575c6e1dca2f"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG4ERp16MazY"
      },
      "source": [
        "def lemmatizer(data):\n",
        "    pos = pos_tag(data)\n",
        "    data = ' '.join([lem.lemmatize(word, po[0].lower()) \n",
        "                      if (po[0].lower() in ['n', 'r', 'v', 'a'] and word[0] != '@') else word for word, po in pos])\n",
        "    return data\n",
        "data_copy['lemmatized'] = data_copy['Tweet_tokenized'].apply(lambda x: lemmatizer(x))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSflhxbXpEjX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vn9WzNY_2V-"
      },
      "source": [
        "def _analyzer (data):\n",
        "    data  = _clean_tweet(data)\n",
        "    data = _remove_punctuation_numbers(data)\n",
        "    data = _characters(data)\n",
        "    data = _lower(data)\n",
        "    data = remove_stopwords(data)\n",
        "    data = tokenization(data)\n",
        "    data = lemmatizer(data)\n",
        "    return data"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMN9rqG3_55d"
      },
      "source": [
        "# data['cleaned'] = data['message'].apply(lambda x: _analyzer(x))\n",
        "# test['cleaned'] = test['message'].apply(lambda x: _analyzer(x))"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "hf57MHh7AKpL",
        "outputId": "99a4d259-a1d1-4632-9a16-f815a60cfa49"
      },
      "source": [
        "data_copy.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>clean_tweets</th>\n",
              "      <th>clean_punc</th>\n",
              "      <th>clean_char</th>\n",
              "      <th>lower</th>\n",
              "      <th>Tweet_nonstop</th>\n",
              "      <th>Tweet_tokenized</th>\n",
              "      <th>lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "      <td>625221</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "      <td>PolySciMajor EPA chief doesnt think carbon dio...</td>\n",
              "      <td>PolySciMajor EPA chief doesnt think carbon dio...</td>\n",
              "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
              "      <td>polyscimajor epa chief not think carbon dioxid...</td>\n",
              "      <td>[polyscimajor, epa, chief, not, think, carbon,...</td>\n",
              "      <td>polyscimajor epa chief not think carbon dioxid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "      <td>126103</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "      <td>Its not like we lack evidence of anthropogenic...</td>\n",
              "      <td>Its not like we lack evidence of anthropogenic...</td>\n",
              "      <td>its not like we lack evidence of anthropogenic...</td>\n",
              "      <td>its not like we lack evidence anthropogenic gl...</td>\n",
              "      <td>[its, not, like, we, lack, evidence, anthropog...</td>\n",
              "      <td>its not like we lack evidence anthropogenic gl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
              "      <td>698562</td>\n",
              "      <td>: Researchers say we have three years to act o...</td>\n",
              "      <td>Researchers say we have three years to act on...</td>\n",
              "      <td>Researchers say we have three years to act on...</td>\n",
              "      <td>researchers say we have three years to act on...</td>\n",
              "      <td>researchers say we three years act climate cha...</td>\n",
              "      <td>[researchers, say, we, three, years, act, clim...</td>\n",
              "      <td>researcher say we three year act climate chang...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
              "      <td>573736</td>\n",
              "      <td>WIRED : was a pivotal year in the war on clima...</td>\n",
              "      <td>WIRED  was a pivotal year in the war on climat...</td>\n",
              "      <td>WIRED  was a pivotal year in the war on climat...</td>\n",
              "      <td>wired  was a pivotal year in the war on climat...</td>\n",
              "      <td>wired pivotal year in war climate change</td>\n",
              "      <td>[wired, pivotal, year, in, war, climate, change]</td>\n",
              "      <td>wire pivotal year in war climate change</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
              "      <td>466954</td>\n",
              "      <td>: It's , and a racist, sexist, climate change ...</td>\n",
              "      <td>Its  and a racist sexist climate change denyi...</td>\n",
              "      <td>Its  and a racist sexist climate change denyi...</td>\n",
              "      <td>its  and a racist sexist climate change denyi...</td>\n",
              "      <td>its and racist sexist climate change denying b...</td>\n",
              "      <td>[its, and, racist, sexist, climate, change, de...</td>\n",
              "      <td>its and racist sexist climate change deny bigo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                         lemmatized\n",
              "0          1  ...  polyscimajor epa chief not think carbon dioxid...\n",
              "1          1  ...  its not like we lack evidence anthropogenic gl...\n",
              "2          2  ...  researcher say we three year act climate chang...\n",
              "3          1  ...            wire pivotal year in war climate change\n",
              "4          1  ...  its and racist sexist climate change deny bigo...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i6GF55VzbJs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bef2ucHPGQeQ"
      },
      "source": [
        "# tfid_vec = TfidfVectorizer(use_idf=True,min_df= .01 , max_df=0.95 , ngram_range=(1, 2), analyzer='word')\n",
        "# tfid_Vectorized = tfid_vec.fit_transform(data['cleaned']).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Uaqg4r4JeAn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl82uuRDKKUu"
      },
      "source": [
        "from sklearn.utils import resample\n",
        "class_size = int(len(data[data['sentiment']==1])/2)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckLNnkZQKM8g"
      },
      "source": [
        "class_1 = data[data['sentiment']==-1]\n",
        "class_2 = data[data['sentiment']==0]\n",
        "class_3 = data[data['sentiment']==1]\n",
        "class_4 = data[data['sentiment']==2]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_BFoP36aGNZ"
      },
      "source": [
        "# upsampling classes 1, 2, and 4 & downsampling class 3\n",
        "class_1_up = resample(class_1,replace=True,n_samples=class_size, random_state=27)\n",
        "class_2_up = resample(class_2,replace=True,n_samples=class_size, random_state=27)\n",
        "class_4_up = resample(class_4,replace=True,n_samples=class_size, random_state=27)\n",
        "class_3_down = resample(class_3,replace=False,n_samples=class_size, random_state=27)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzOZ-fSzaXe9"
      },
      "source": [
        "# Creating a new DataFrame out of the balanced bata\n",
        "resampled = pd.concat([class_1_up, class_2_up, class_4_up,class_3_down])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP9XwUH9bTda"
      },
      "source": [
        "X = resampled['message'].apply(lambda x: _analyzer(x))\n",
        "y = resampled['sentiment']"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWuWabPwKZZF"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train , X_test , y_train , y_test = train_test_split(X , y, stratify=y,\n",
        "                                                       test_size =0.2, \n",
        "                                                       random_state=42)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y16tgeguFMUH"
      },
      "source": [
        "# Models\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5pPson0Kf7G"
      },
      "source": [
        "classifiers = [LogisticRegression(random_state=42), SVC(kernel='poly', random_state = 42), \n",
        "               SVC(kernel = 'rbf', random_state = 42),MultinomialNB(),RidgeClassifier(),\n",
        "               LinearSVC(random_state=42), SGDClassifier(random_state=42), RandomForestClassifier(random_state=42)]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edofgZ2SNI3Z"
      },
      "source": [
        "def _performace_assesment(*args , **kwargs):\n",
        "  model_stats = {}\n",
        "  for clf in classifiers:\n",
        "    model = Pipeline([('count_vec', TfidfVectorizer(lowercase = True, ngram_range=(1, 2), analyzer='word')),\n",
        "                      ('clf' , clf)\n",
        "                      ])\n",
        "    \n",
        "    model.fit(X_train, y_train) #Training\n",
        "    model_pred = model.predict(X_test) #Testing\n",
        "\n",
        "    # Dictionary of Models Performances\n",
        "    model_stats[clf.__class__.__name__] = {\n",
        "        'F1-Macro':metrics.f1_score(y_test, model_pred, average='macro'),\n",
        "        'F1-Accuracy':metrics.f1_score(y_test, model_pred, average='micro'),\n",
        "        'F1-Weighted':metrics.f1_score(y_test, model_pred, average='weighted')}\n",
        "  return pd.DataFrame.from_dict(model_stats, orient='index')\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "h9uQE3_kNiXo",
        "outputId": "0bfcba8d-a8ae-402e-86f1-8540883f9318"
      },
      "source": [
        "performance = _performace_assesment(classifiers , X_train , X_test , y_train , y_test)\n",
        "performance.to_csv('performance.csv')\n",
        "dataframe = pd.read_csv('performance.csv', index_col = 0)\n",
        "dataframe.sort_values('F1-Weighted', ascending=False)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>F1-Macro</th>\n",
              "      <th>F1-Accuracy</th>\n",
              "      <th>F1-Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SVC</th>\n",
              "      <td>0.876726</td>\n",
              "      <td>0.876905</td>\n",
              "      <td>0.876726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LinearSVC</th>\n",
              "      <td>0.857236</td>\n",
              "      <td>0.860199</td>\n",
              "      <td>0.857236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RidgeClassifier</th>\n",
              "      <td>0.853112</td>\n",
              "      <td>0.855803</td>\n",
              "      <td>0.853112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SGDClassifier</th>\n",
              "      <td>0.845635</td>\n",
              "      <td>0.849062</td>\n",
              "      <td>0.845635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LogisticRegression</th>\n",
              "      <td>0.834257</td>\n",
              "      <td>0.837632</td>\n",
              "      <td>0.834257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForestClassifier</th>\n",
              "      <td>0.833990</td>\n",
              "      <td>0.838511</td>\n",
              "      <td>0.833990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MultinomialNB</th>\n",
              "      <td>0.807196</td>\n",
              "      <td>0.812134</td>\n",
              "      <td>0.807196</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        F1-Macro  F1-Accuracy  F1-Weighted\n",
              "SVC                     0.876726     0.876905     0.876726\n",
              "LinearSVC               0.857236     0.860199     0.857236\n",
              "RidgeClassifier         0.853112     0.855803     0.853112\n",
              "SGDClassifier           0.845635     0.849062     0.845635\n",
              "LogisticRegression      0.834257     0.837632     0.834257\n",
              "RandomForestClassifier  0.833990     0.838511     0.833990\n",
              "MultinomialNB           0.807196     0.812134     0.807196"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdp5EXSANoPr"
      },
      "source": [
        "def _param_tuning(*args , **kwargs):\n",
        "  best_params = {}\n",
        "\n",
        "  for clf in classifiers:\n",
        "    model = Pipeline([('count_vec', TfidfVectorizer(lowercase = True, ngram_range=(1, 2), analyzer='word')),\n",
        "                      ('clf' , clf)\n",
        "                      ])\n",
        "    model.fit(X_train, y_train) #Training\n",
        "    \n",
        "    #Get models performing parameters\n",
        "    params = model.get_params()\n",
        "    model_name = clf.__class__.__name__ \n",
        "    model_name = {}\n",
        "    for key in params:\n",
        "      if key.startswith(\"clf\"):\n",
        "        if len(key) < 5:\n",
        "          model_name['model'] = params[key]\n",
        "        else:\n",
        "            model_name[key[5:]] = params[key]\n",
        "    best_params[clf.__class__.__name__] = model_name\n",
        "  return best_params"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocRC81iKPC53"
      },
      "source": [
        "best_params = _param_tuning(classifiers, X_train, y_train)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq1BLCf5POSG",
        "outputId": "78c9ca42-cb40-4924-acff-12c6acdc549b"
      },
      "source": [
        "#Best parameters\n",
        "best_params"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'LinearSVC': {'C': 1.0,\n",
              "  'class_weight': None,\n",
              "  'dual': True,\n",
              "  'fit_intercept': True,\n",
              "  'intercept_scaling': 1,\n",
              "  'loss': 'squared_hinge',\n",
              "  'max_iter': 1000,\n",
              "  'model': LinearSVC(random_state=42),\n",
              "  'multi_class': 'ovr',\n",
              "  'penalty': 'l2',\n",
              "  'random_state': 42,\n",
              "  'tol': 0.0001,\n",
              "  'verbose': 0},\n",
              " 'LogisticRegression': {'C': 1.0,\n",
              "  'class_weight': None,\n",
              "  'dual': False,\n",
              "  'fit_intercept': True,\n",
              "  'intercept_scaling': 1,\n",
              "  'l1_ratio': None,\n",
              "  'max_iter': 100,\n",
              "  'model': LogisticRegression(random_state=42),\n",
              "  'multi_class': 'auto',\n",
              "  'n_jobs': None,\n",
              "  'penalty': 'l2',\n",
              "  'random_state': 42,\n",
              "  'solver': 'lbfgs',\n",
              "  'tol': 0.0001,\n",
              "  'verbose': 0,\n",
              "  'warm_start': False},\n",
              " 'MultinomialNB': {'alpha': 1.0,\n",
              "  'class_prior': None,\n",
              "  'fit_prior': True,\n",
              "  'model': MultinomialNB()},\n",
              " 'RandomForestClassifier': {'bootstrap': True,\n",
              "  'ccp_alpha': 0.0,\n",
              "  'class_weight': None,\n",
              "  'criterion': 'gini',\n",
              "  'max_depth': None,\n",
              "  'max_features': 'auto',\n",
              "  'max_leaf_nodes': None,\n",
              "  'max_samples': None,\n",
              "  'min_impurity_decrease': 0.0,\n",
              "  'min_samples_leaf': 1,\n",
              "  'min_samples_split': 2,\n",
              "  'min_weight_fraction_leaf': 0.0,\n",
              "  'model': RandomForestClassifier(random_state=42),\n",
              "  'n_estimators': 100,\n",
              "  'n_jobs': None,\n",
              "  'oob_score': False,\n",
              "  'random_state': 42,\n",
              "  'verbose': 0,\n",
              "  'warm_start': False},\n",
              " 'RidgeClassifier': {'alpha': 1.0,\n",
              "  'class_weight': None,\n",
              "  'copy_X': True,\n",
              "  'fit_intercept': True,\n",
              "  'max_iter': None,\n",
              "  'model': RidgeClassifier(),\n",
              "  'normalize': 'deprecated',\n",
              "  'positive': False,\n",
              "  'random_state': None,\n",
              "  'solver': 'auto',\n",
              "  'tol': 0.001},\n",
              " 'SGDClassifier': {'alpha': 0.0001,\n",
              "  'average': False,\n",
              "  'class_weight': None,\n",
              "  'early_stopping': False,\n",
              "  'epsilon': 0.1,\n",
              "  'eta0': 0.0,\n",
              "  'fit_intercept': True,\n",
              "  'l1_ratio': 0.15,\n",
              "  'learning_rate': 'optimal',\n",
              "  'loss': 'hinge',\n",
              "  'max_iter': 1000,\n",
              "  'model': SGDClassifier(random_state=42),\n",
              "  'n_iter_no_change': 5,\n",
              "  'n_jobs': None,\n",
              "  'penalty': 'l2',\n",
              "  'power_t': 0.5,\n",
              "  'random_state': 42,\n",
              "  'shuffle': True,\n",
              "  'tol': 0.001,\n",
              "  'validation_fraction': 0.1,\n",
              "  'verbose': 0,\n",
              "  'warm_start': False},\n",
              " 'SVC': {'C': 1.0,\n",
              "  'break_ties': False,\n",
              "  'cache_size': 200,\n",
              "  'class_weight': None,\n",
              "  'coef0': 0.0,\n",
              "  'decision_function_shape': 'ovr',\n",
              "  'degree': 3,\n",
              "  'gamma': 'scale',\n",
              "  'kernel': 'rbf',\n",
              "  'max_iter': -1,\n",
              "  'model': SVC(random_state=42),\n",
              "  'probability': False,\n",
              "  'random_state': 42,\n",
              "  'shrinking': True,\n",
              "  'tol': 0.001,\n",
              "  'verbose': False}}"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpZyx_F4s90Z"
      },
      "source": [
        "model = SVC(kernel='poly', random_state = 42)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFGXfoqbwQTa"
      },
      "source": [
        "Vectorize = TfidfVectorizer(lowercase = True, ngram_range=(1, 2), analyzer='word')\n",
        "X_train = Vectorize.fit_transform(X_train)\n",
        "X_test = Vectorize.transform(X_test)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqGQjJPvw33M"
      },
      "source": [
        "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True,\n",
        "                                   random_state=42)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEYrK2ijw_08",
        "outputId": "0b222496-490b-4d02-de8d-f44cf7ba533e"
      },
      "source": [
        "best_params[classifiers[2].__class__.__name__]"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 1.0,\n",
              " 'break_ties': False,\n",
              " 'cache_size': 200,\n",
              " 'class_weight': None,\n",
              " 'coef0': 0.0,\n",
              " 'decision_function_shape': 'ovr',\n",
              " 'degree': 3,\n",
              " 'gamma': 'scale',\n",
              " 'kernel': 'rbf',\n",
              " 'max_iter': -1,\n",
              " 'model': SVC(random_state=42),\n",
              " 'probability': False,\n",
              " 'random_state': 42,\n",
              " 'shrinking': True,\n",
              " 'tol': 0.001,\n",
              " 'verbose': False}"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F6BiO4j18OY"
      },
      "source": [
        "# param_grid = {\n",
        "#     \"average\": [True, False],\n",
        "#     \"l1_ratio\": np.linspace(0, 1, num=10),\n",
        "#     \"alpha\": np.power(10, np.arange(-4, 1, dtype=float)),\n",
        "# }\n",
        "\n",
        "param_grid = {'kernel': ('linear', 'rbf'),'C': [1, 10, 100]}\n",
        "grid_search = GridSearchCV(estimator= model,\n",
        "                           param_grid=param_grid,\n",
        "                           scoring='f1_weighted',\n",
        "                           cv=stratified_kfold,\n",
        "                           error_score=0,\n",
        "                           n_jobs=-1)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97hIobmdyjGb"
      },
      "source": [
        "grid_search.fit(X_train, y_train)\n",
        "prediction = grid_search.predict(X_test)\n",
        "cv_score = grid_search.best_score_\n",
        "test_score = grid_search.score(X_test, y_test)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK8R6L5LxG9H",
        "outputId": "87c9a721-e86b-45aa-e97b-8c038c4475a7"
      },
      "source": [
        "grid_search.get_params().keys()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['cv', 'error_score', 'estimator__C', 'estimator__class_weight', 'estimator__dual', 'estimator__fit_intercept', 'estimator__intercept_scaling', 'estimator__l1_ratio', 'estimator__max_iter', 'estimator__multi_class', 'estimator__n_jobs', 'estimator__penalty', 'estimator__random_state', 'estimator__solver', 'estimator__tol', 'estimator__verbose', 'estimator__warm_start', 'estimator', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4DUHSnpylkK",
        "outputId": "a2b5c1fc-0a0e-4f92-84d9-ffa4438966d0"
      },
      "source": [
        "print(f'Cross-validation score: {cv_score}')\n",
        "print(f'Test score: {test_score}')\n",
        "grid_search.best_params_    \n",
        "grid_search.best_estimator_"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation score: 0.882580752249323\n",
            "Test score: 0.8803448226416891\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=100, random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxvDEyqpvfDY"
      },
      "source": [
        "X_train , X_test , y_train , y_test = train_test_split(X , y, stratify=y,\n",
        "                                                       test_size =0.2, \n",
        "                                                       random_state=42)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffDVIPw3vnHo"
      },
      "source": [
        "Vectorize = TfidfVectorizer(lowercase = True, ngram_range=(1, 2), analyzer='word')\n",
        "X_train = Vectorize.fit_transform(X_train)\n",
        "X_test = Vectorize.transform(X_test)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g_1ET7vvsHS"
      },
      "source": [
        "log_sgd1 = LogisticRegression(C=5)\n",
        "log_sgd2 = LogisticRegression(C=5)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaDa3OilxA3z"
      },
      "source": [
        "estimators = [('log_sgd1', log_sgd1), ('log_sgd2', log_sgd2)]\n",
        "final_est = RidgeClassifier(alpha=0.2125)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqJh4hFS_7Lj"
      },
      "source": [
        "from sklearn.ensemble import StackingClassifier"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Laa2lyH-xFwx"
      },
      "source": [
        "stacking_log2 = StackingClassifier(estimators = estimators,\n",
        "                           final_estimator = final_est,\n",
        "                           passthrough = True)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU0ghV-exOxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4b3838-a8b5-4ec2-84ef-7fb9d76905f8"
      },
      "source": [
        "stacking_log2.fit(X_train , y_train)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingClassifier(estimators=[('log_sgd1', LogisticRegression(C=5)),\n",
              "                               ('log_sgd2', LogisticRegression(C=5))],\n",
              "                   final_estimator=RidgeClassifier(alpha=0.2125),\n",
              "                   passthrough=True)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1vNZ525xTV3"
      },
      "source": [
        "pred = stacking_log2.predict(X_test)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwiOY40xxauL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "73da34a5-ee0d-443f-e61f-e206d79565de"
      },
      "source": [
        "model_stats = {}\n",
        "model_stats[stacking_log2.__class__.__name__] = {\n",
        "        'F1-Macro':metrics.f1_score(y_test, pred, average='macro'),\n",
        "        'F1-Accuracy':metrics.f1_score(y_test, pred, average='micro'),\n",
        "        'F1-Weighted':metrics.f1_score(y_test, pred, average='weighted')}\n",
        "pd.DataFrame.from_dict(model_stats, orient='index')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>F1-Macro</th>\n",
              "      <th>F1-Accuracy</th>\n",
              "      <th>F1-Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>StackingClassifier</th>\n",
              "      <td>0.846806</td>\n",
              "      <td>0.849941</td>\n",
              "      <td>0.846806</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    F1-Macro  F1-Accuracy  F1-Weighted\n",
              "StackingClassifier  0.846806     0.849941     0.846806"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUrBM1vZ11Q0"
      },
      "source": [
        "X = test['message'].apply(lambda x: _analyzer(x))\n",
        "Vector = Vectorize.transform(X)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4M2CdVXIP7N"
      },
      "source": [
        "model = SVC(C=100, random_state=42).fit(X_train , y_train)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxrqdfKZFO6Q"
      },
      "source": [
        "pred = model.predict(Vector)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "OtvHCSszFnzb",
        "outputId": "d335988e-e955-42b0-ee4c-3abcd5842b69"
      },
      "source": [
        "daf = pd.DataFrame(pred, columns=['sentiment'])\n",
        "daf.head(20)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sentiment\n",
              "0           1\n",
              "1           1\n",
              "2           1\n",
              "3           1\n",
              "4           0\n",
              "5           2\n",
              "6           1\n",
              "7           1\n",
              "8           1\n",
              "9           2\n",
              "10         -1\n",
              "11          1\n",
              "12          2\n",
              "13          1\n",
              "14          1\n",
              "15          0\n",
              "16          1\n",
              "17          1\n",
              "18         -1\n",
              "19          2"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "AgeYfP00Kcs6",
        "outputId": "5138d82e-a4a6-4805-e196-95164ad934fc"
      },
      "source": [
        "output = pd.DataFrame({\"tweetid\":test['tweetid']})\n",
        "submission = output.join(daf)        \n",
        "submission.to_csv(\"submission3.csv\", index=False)\n",
        "submission"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweetid</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>169760</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>35326</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>224985</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>476263</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>872928</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10541</th>\n",
              "      <td>895714</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10542</th>\n",
              "      <td>875167</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10543</th>\n",
              "      <td>78329</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10544</th>\n",
              "      <td>867455</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10545</th>\n",
              "      <td>470892</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10546 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       tweetid  sentiment\n",
              "0       169760          1\n",
              "1        35326          1\n",
              "2       224985          1\n",
              "3       476263          1\n",
              "4       872928          0\n",
              "...        ...        ...\n",
              "10541   895714          0\n",
              "10542   875167          1\n",
              "10543    78329          0\n",
              "10544   867455          0\n",
              "10545   470892          1\n",
              "\n",
              "[10546 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}